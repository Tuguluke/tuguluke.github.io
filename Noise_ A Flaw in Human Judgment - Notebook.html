<!DOCTYPE html PUBLIC
"-//W3C//DTD XHTML 1.0 Strict//EN"
"XHTML1-s.dtd" >
<html xmlns="http://www.w3.org/TR/1999/REC-html-in-xml" xml:lang="en" lang="en">
    <head>
    <meta charset="UTF-8">
    <style>
        .bodyContainer {
            font-family: Arial, Helvetica, sans-serif;
            text-align: center;
            padding-left: 32px;
            padding-right: 32px;
        }
        
        .notebookFor {
            font-size: 18px;
            font-weight: 700;
            text-align: center;
            color: rgb(119, 119, 119);
            margin: 24px 0px 0px;
            padding: 0px;
        }
        
        .bookTitle {
            font-size: 24px;
            font-weight: 700;
            text-align: center;
            color: #333333;
            margin-top: 22px;
            padding: 0px;
        }
        
        .authors {
            font-size: 18px;
            font-weight: 700;
            text-align: center;
            color: rgb(119, 119, 119);
            margin-top: 22px;
            margin-bottom: 24px;
            padding: 0px;
        }
    
        .citation {
            font-size: 18px;
            font-weight: 500;
            text-align: center;
            color: #333333;
            margin-top: 22px;
            margin-bottom: 24px;
            padding: 0px;
        }
    
        .sectionHeading {
            font-size: 24px;
            font-weight: 700;
            text-align: left;
            color: #333333;
            margin-top: 24px;
            padding: 0px;
        }
        
        .noteHeading {
            font-size: 18px;
            font-weight: 700;
            text-align: left;
            color: #333333;
            margin-top: 20px;
            padding: 0px;
        }
        
        .noteText {
            font-size: 18px;
            font-weight: 500;
            text-align: left;
            color: #333333;
            margin: 2px 0px 0px;
            padding: 0px;
        }
        
        .highlight_blue {
            color: rgb(178, 205, 251);
        }
        
        .highlight_orange {
            color: #ffd7ae;
        }
        
        .highlight_pink {
            color: rgb(255, 191, 206);
        }
        
        .highlight_yellow {
            color: rgb(247, 206, 0);
        }
        
        .notebookGraphic {
            margin-top: 10px;
            text-align: left;
        }
        
        .notebookGraphic img {
            -o-box-shadow:      0px 0px 5px #888;
            -icab-box-shadow:   0px 0px 5px #888;
            -khtml-box-shadow:  0px 0px 5px #888;
            -moz-box-shadow:    0px 0px 5px #888;
            -webkit-box-shadow: 0px 0px 5px #888;
            box-shadow:         0px 0px 5px #888; 
            max-width: 100%;
            height: auto;
        }
        
        hr {
            border: 0px none;
            height: 1px;
            background: none repeat scroll 0% 0% rgb(221, 221, 221);
        }
        </style>
        <script>
            </script>
    </head>
    <body>
        <div class="bodyContainer">
            <div class="notebookFor">
                Notebook Export
            </div>
            <div class="bookTitle">
                Noise: A Flaw in Human Judgment
            </div>
            <div class="authors">
                Kahneman, Daniel; Sibony, Olivier; Sunstein, Cass R.
            </div>
            <div class="citation">
                
            </div>
            <hr />
            <div class="sectionHeading">
    Introduction: Two Kinds of Error
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 4 · Location 91
</div>
<div class="noteText">
    Bias and noise—systematic deviation and random scatter—are different components of error.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 5 · Location 100
</div>
<div class="noteText">
    A general property of noise is that you can recognize and measure it while knowing nothing about the target or bias.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 5 · Location 108
</div>
<div class="noteText">
    To understand error in judgment, we must understand both bias and noise.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 6 · Location 114
</div>
<div class="noteText">
    Noise is especially high in psychiatry, where subjective judgment is obviously important.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 7 · Location 127
</div>
<div class="noteText">
    Personnel decisions are noisy. Interviewers of job candidates make widely different assessments of the same people. Performance ratings of the same employees are also highly variable and depend more on the person doing the assessment than on the performance being assessed.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 8 · Location 144
</div>
<div class="noteText">
    To establish that point, we introduce the idea of a noise audit, designed to measure how much disagreement there is among professionals considering the same cases within an organization.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 8 · Location 147
</div>
<div class="noteText">
    Occasion noise is the variability in judgments of the same case by the same person or group on different occasions. A surprising amount of occasion noise arises in group discussion because of seemingly irrelevant factors, such as who speaks first.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 8 · Location 151
</div>
<div class="noteText">
    We discuss the ultimate limit on the quality of predictive judgment—objective ignorance of the future—and how it conspires with noise to limit the quality of prediction.
</div><div class="sectionHeading">
    Part I: Finding Noise
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 12 · Location 196
</div>
<div class="noteText">
    which will be a key theme of this book: wherever there is judgment, there is noise—and more of it than you think.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - 1. Crime and Noisy Punishment > Page 17 · Location 264
</div>
<div class="noteText">
    A review of 207,000 immigration court decisions over four years found a significant effect of daily temperature variations: when it is hot outside, people are less likely to get asylum. If you are suffering political persecution in your home country and want asylum elsewhere, you should hope and maybe even pray that your hearing falls on a cool day.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - 1. Crime and Noisy Punishment > Page 21 · Location 315
</div>
<div class="noteText">
    At the same time, female judges became more likely than male judges were to exercise their increased discretion in favor of leniency. The same is true of judges appointed by Democratic presidents.
</div><div class="sectionHeading">
    Part II: Your Mind Is a Measuring Instrument
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 39 · Location 570
</div>
<div class="noteText">
    Judgment can therefore be described as measurement in which the instrument is a human mind.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 40 · Location 574
</div>
<div class="noteText">
    Judgment is not a synonym for thinking, and making accurate judgments is not a synonym for having good judgment.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - 5. Measuring Error > Page 62 · Location 869
</div>
<div class="noteText">
    The role of bias and noise in error is easily summarized in two expressions that we will call the error equations. The first of these equations decomposes the error in a single measurement into the two components with which you are now familiar: bias—the average error—and a residual “noisy error.” The noisy error is positive when the error is larger than the bias, negative when it is smaller. The average of noisy errors is zero. Nothing new in the first error equation.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - 5. Measuring Error > Page 62 · Location 873
</div>
<div class="noteText">
    Error in a single measurement = Bias + Noisy Error
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - 5. Measuring Error > Page 62 · Location 876
</div>
<div class="noteText">
    Overall Error (MSE) = Bias2 + Noise2
</div><div class="sectionHeading">
    Review and Conclusion: Taking Noise Seriously
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 361 · Location 5158
</div>
<div class="noteText">
    REVIEW AND CONCLUSION Taking Noise Seriously Noise is the unwanted variability of judgments, and there is too much of it. Our central goals here have been to explain why that is so and to see what might be done about it. We have covered a great deal of material in this book, and by way of conclusion, we offer here a brisk review of the main points, as well as a broader perspective. Judgments As we use the term, judgment should not be confused with “thinking.” It is a much narrower concept: judgment is a form of measurement in which the instrument is a human mind. Like other measurements, a judgment assigns a score to an object. The score need not be a number. “Mary Johnson’s tumor is probably benign” is a judgment, as are statements like “The national economy is very unstable,” “Fred Williams would be the best person to hire as our new manager,” and “The premium to insure this risk should be $ 12,000.” Judgments informally integrate diverse pieces of information into an overall assessment. They are not computations, and they do not follow exact rules. A teacher uses judgment to grade an essay, but not to score a multiple-choice test. Many people earn a living by making professional judgments, and everyone is affected by such judgments in important ways. Professional judges, as we call them here, include football coaches and cardiologists, lawyers and engineers, Hollywood executives and insurance underwriters, and many more. Professional judgments have been the focus of this book, both because they have been extensively studied and because their quality has such a large impact on all of us. We believe that what we have learned applies to judgments that people make in other parts of their lives, too.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 362 · Location 5172
</div>
<div class="noteText">
    Some judgments are predictive, and some predictive judgments are verifiable; we will eventually know whether they were accurate. This is generally the case for short-term forecasts of outcomes such as the effects of a medication, the course of a pandemic, or the results of an election. But many judgments, including long-term forecasts and answers to fictitious questions, are unverifiable. The quality of such judgments can be assessed only by the quality of the thought process that produces them. Furthermore, many judgments are not predictive but evaluative: the sentence set by a judge or the rank of a painting in a prize competition cannot easily be compared to an objective true value. Strikingly, however, people who make judgments behave as if a true value exists, regardless of whether it does. They think and act as if there were an invisible bull’s-eye at which to aim, one that they and others should not miss by much. The phrase judgment call implies both the possibility of disagreement and the expectation that it will be limited. Matters of judgment are characterized by an expectation of bounded disagreement. They occupy a space between matters of computation, where disagreement is not allowed, and matters of taste, where there is little expectation of agreement except in extreme cases. Errors: Bias and Noise We say that bias exists when most errors in a set of judgments are in the same direction. Bias is the average error, as, for example, when a team of shooters consistently hits below and to the left of the target; when executives are too optimistic about sales, year after year; or when a company keeps reinvesting money in failing projects that it should write off. Eliminating bias from a set of judgments will not eliminate all error. The errors that remain when bias is removed are not shared. They are the unwanted divergence of judgments, the unreliability of the measuring instrument we apply to reality. They are noise. Noise is variability in judgments that should be identical. We use the term system noise for the noise observed in organizations that employ interchangeable professionals to make decisions, such as physicians in an emergency room, judges imposing criminal penalties, and underwriters in an insurance company. Much of this book has been concerned with system noise. Measuring Bias and Noise The mean of squared errors (MSE) has been the standard of accuracy in scientific measurement for two hundred years. The main features of MSE are that it yields the sample mean as an unbiased estimate of the population mean, treats positive and negative errors equally, and disproportionately penalizes large errors. MSE does not reflect the real costs of judgment errors, which are often asymmetric. However, professional decisions always require accurate predictions. For a city facing a hurricane, the costs of under-and overestimating the threat are clearly not the same, but you would not want these costs to influence the meteorologists’ forecast of the storm’s speed and trajectory. MSE is the appropriate standard for making such predictive judgments, where objective accuracy is the goal. As measured by MSE, bias and noise are independent and additive sources of error. Obviously, bias is always bad and reducing it always improves accuracy. Less intuitive is the fact that noise is equally bad and that reducing noise is always an improvement. The best amount of scatter is zero, even when the judgments are clearly biased. The goal, of course, is to minimize both bias and noise. Bias in a set of verifiable judgments is defined by the difference between the average judgment of a case and the corresponding true value. This comparison is impossible for unverifiable judgments. For example, the true value of a premium that an underwriter sets for a particular risk will never be known. Nor can we easily know the true value of the just sentence for a particular crime. Lacking that knowledge, a frequent and convenient (though not always correct) assumption is that judgments are unbiased and that the average of many judges is the best estimate of the true value. Noise in a system can be assessed by a noise audit, an experiment in which several professionals make independent judgments of the same cases (real or fictitious). We can measure noise without knowing a true value, just as we can see, from the back of the target, the scatter of a set of shots. Noise audits can measure the variability of judgments in many systems, including a radiology department and the system of criminal justice. They may sometimes call attention to deficiencies in skill or training. And they will quantify system noise—for instance, when underwriters in the same team differ in their assessments of risks. Of bias and noise, which is the larger problem? It depends on the situation. The answer might well turn out to be noise. Bias and noise make equal contributions to overall error (MSE) when the mean of errors (the bias) is equal to the standard deviations of errors (the noise). When the distribution of judgments is normal (the standard bell-shaped curve), the effects of bias and noise are equal when 84% of judgments are above (or below) the true value. This is a substantial bias, which will often be detectable in a professional context. When the bias is smaller than one standard deviation, noise is the bigger source of overall error. Noise Is a Problem Variability as such is unproblematic in some judgments, even welcome. Diversity of opinions is essential for generating ideas and options. Contrarian thinking is essential to innovation. A plurality of opinions among movie critics is a feature, not a bug. Disagreements among traders make markets. Strategy differences among competing start-ups enable markets to select the fittest. In what we call matters of judgment, however, system noise is always a problem. If two doctors give you different diagnoses, at least one of them is wrong. The surprises that motivated this book are the sheer magnitude of system noise and the amount of damage that it does. Both of these far exceed common expectations. We have given examples from many fields, including business, medicine, criminal justice, fingerprint analysis, forecasting, personnel ratings, and politics. Hence our conclusion: wherever there is judgment, there is noise, and more of it than you think. The large role of noise in error contradicts a commonly held belief that random errors do not matter, because they “cancel out.” This belief is wrong. If multiple shots are scattered around the target, it is unhelpful to say that, on average, they hit the bull’s-eye. If one candidate for a job gets a higher rating than she deserves and another gets a lower one, the wrong person may be hired. If one insurance policy is overpriced and another is underpriced, both errors are costly to the insurance company; one makes it lose business, the other makes it lose money. In short, we can be sure that there is error if judgments vary for no good reason. Noise is detrimental even when judgments are not verifiable and error cannot be measured. It is unfair for similarly situated people to be treated differently, and a system in which professional judgments are seen as inconsistent loses credibility. Types of Noise System noise can be broken down into level noise and pattern noise. Some judges are generally more severe than others, and others are more lenient; some forecasters are generally bullish and others bearish about market prospects; some doctors prescribe more antibiotics than others do. Level noise is the variability of the average judgments made by different individuals. The ambiguity of judgment scales is one of the sources of level noise. Words such as likely or numbers (e.g., “4 on a scale of 0 to 6”) mean different things to different people. Level noise is an important source of error in judgment systems and an important target for interventions aimed at noise reduction. System noise includes another, generally larger component. Regardless of the average level of their judgments, two judges may differ in their views of which crimes deserve the harsher sentences. Their sentencing decisions will produce a different ranking of cases. We call this variability pattern noise (the technical term is statistical interaction). The main source of pattern noise is stable: it is the difference in the personal, idiosyncratic responses of judges to the same case. Some of these differences reflect principles or values that the individuals follow, whether consciously or not. For example, one judge might be especially severe with shoplifters and unusually lenient with traffic offenders; another might show the opposite pattern. Some of the underlying principles or values may be quite complex, and the judge may be unaware of them. For example, a judge could be relatively lenient toward older shoplifters without realizing it. Finally, a highly personal reaction to a particular case could also be stable. A defendant who resembles the judge’s daughter might well have evoked the same feeling of sympathy, and hence leniency, on another day. This stable pattern noise reflects the uniqueness of judges: their response to cases is as individual as their personality. The subtle differences among people are often enjoyable and interesting, but the differences become problematic when professionals operate within a system that assumes consistency. In the studies we have examined, the stable pattern noise that such individual differences produce is generally the single largest source of system noise. Still, judges’ distinctive attitudes to particular cases are not perfectly stable. Pattern noise also has a transient component, called occasion noise. We detect this kind of noise if a radiologist assigns different diagnoses to the same image on different days or if a fingerprint examiner identifies two prints as a match on one occasion but not on another. As these examples illustrate, occasion noise is most easily measured when the judge does not recognize the case as one seen before. Another way to demonstrate occasion noise is to show the effect of an irrelevant feature of the context on judgments, such as when judges are more lenient after their favorite football team won, or when doctors prescribe more opioids in the afternoon. The Psychology of Judgment and Noise The judges’ cognitive flaws are not the only cause of errors in predictive judgments. Objective ignorance often plays a larger role. Some facts are actually unknowable—how many grandchildren a baby born yesterday will have seventy years from now, or the number of a winning lottery ticket in a drawing to be held next year. Others are perhaps knowable but are not known to the judge. People’s exaggerated confidence in their predictive judgment underestimates their objective ignorance as well as their biases. There is a limit to the accuracy of our predictions, and this limit is often quite low. Nevertheless, we are generally comfortable with our judgments. What gives us this satisfying confidence is an internal signal, a self-generated reward for fitting the facts and the judgment into a coherent story. Our subjective confidence in our judgments is not necessarily related to their objective accuracy. Most people are surprised to hear that the accuracy of their predictive judgments is not only low but also inferior to that of formulas. Even simple linear models built on limited data, or simple rules that can be sketched on the back of an envelope, consistently outperform human judges. The critical advantage of rules and models is that they are noise-free. As we subjectively experience it, judgment is a subtle and complex process; we have no indication that the subtlety may be mostly noise. It is difficult for us to imagine that mindless adherence to simple rules will often achieve higher accuracy than we can—but this is by now a well-established fact. Psychological biases are, of course, a source of systematic error, or statistical bias. Less obviously, they are also a source of noise. When biases are not shared by all judges, when they are present to different degrees, and when their effects depend on extraneous circumstances, psychological biases produce noise. For instance, if half the managers who make hiring decisions are biased against women and half are biased in their favor, there will be no overall bias, but system noise will cause many hiring errors. Another example is the disproportionate effect of first impressions. This is a psychological bias, but that bias will produce occasion noise when the order in which the evidence is presented varies randomly. We have described the process of judgment as the informal integration of a set of cues to produce a judgment on a scale. The elimination of system noise would therefore require judges to maintain uniformity in their use of cues, in the weights they assign to cues, and in their use of the scale. Even leaving aside the random effects of occasion noise, these conditions are rarely met. Agreement is often fairly high in judgments on single dimensions. Different recruiters will often agree on their evaluations of which of two candidates is more charismatic or more diligent. The shared intuitive process of matching across intensity dimensions—such as when people match a high GPA to a precocious reading age—will generally produce similar judgments. The same is true of judgments based on a small number of cues that point in the same general direction. Large individual differences emerge when a judgment requires the weighting of multiple, conflicting cues. Looking at the same candidate, some recruiters will give more weight to evidence of brilliance or charisma; others will be more influenced by concerns about diligence or calm under pressure. When cues are inconsistent and do not fit a coherent story, different people will inevitably give more weight to certain cues and ignore others. Pattern noise will result. The Obscurity of Noise Noise is not a prominent problem. It is rarely discussed, and it is certainly less salient than bias. You probably had not given it much thought. Given its importance, the obscurity of noise is an interesting phenomenon in and of itself.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 369 · Location 5276
</div>
<div class="noteText">
    Cognitive biases and other emotional or motivated distortions of thinking are often used as explanations for poor judgments. Analysts invoke overconfidence, anchoring, loss aversion, availability bias, and other biases to explain decisions that turned out badly. Such bias-based explanations are satisfying, because the human mind craves causal explanations. Whenever something goes wrong, we look for a cause—and often find it. In many cases, the cause will appear to be a bias. Bias has a kind of explanatory charisma, which noise lacks. If we try to explain, in hindsight, why a particular decision was wrong, we will easily find bias and never find noise. Only a statistical view of the world enables us to see noise, but that view does not come naturally—we prefer causal stories. The absence of statistical thinking from our intuitions is one reason that noise receives so much less attention than bias does. Another reason is that professionals seldom see a need to confront noise in their own judgments and in those of their colleagues. After a period of training, professionals often make judgments on their own. Fingerprint experts, experienced underwriters, and veteran patent officers rarely take time to imagine how colleagues might disagree with them—and they spend even less time imagining how they might disagree with themselves. Most of the time, professionals have confidence in their own judgment. They expect that colleagues would agree with them, and they never find out whether they actually do. In most fields, a judgment may never be evaluated against a true value and will at most be subjected to vetting by another professional who is considered a respect-expert. Only occasionally will professionals be faced with a surprising disagreement, and when that happens, they will generally find reasons to view it as an isolated case. The routines of organizations also tend to ignore or suppress evidence of divergence among experts in their midst. This is understandable; from an organizational perspective, noise is an embarrassment. How to Reduce Noise (and Bias, Too) There is reason to believe that some people make better judgments than others do. Task-specific skill, intelligence, and a certain cognitive style—best described as being actively open-minded—characterize the best judges. Unsurprisingly, good judges will make few egregious mistakes. Given the multiple sources of individual differences, however, we should not expect even the best judges to be in perfect agreement on complex judgment problems. The infinite variety of backgrounds, personalities, and experiences that make each of us unique is also what makes noise inevitable. One strategy for error reduction is debiasing. Typically, people attempt to remove bias from their judgments either by correcting judgments after the fact or by taming biases before they affect judgments. We propose a third option, which is particularly applicable to decisions made in a group setting: detect biases in real time, by designating a decision observer to identify signs of bias (see appendix B). Our main suggestion for reducing noise in judgment is decision hygiene. We chose this term because noise reduction, like health hygiene, is prevention against an unidentified enemy. Handwashing, for example, prevents unknown pathogens from entering our bodies. In the same way, decision hygiene will prevent errors without knowing what they are. Decision hygiene is as unglamorous as its name and certainly less exciting than a victorious fight against predictable biases. There may be no glory in preventing an unidentified harm, but it is very much worth doing. A noise-reduction effort in an organization should always begin with a noise audit (see appendix A). An important function of the audit is to obtain a commitment of the organization to take noise seriously. An essential benefit is the assessment of separate types of noise. We described the successes and limitations of noise reduction efforts in various domains. We now recapitulate six principles that define decision hygiene, describe how they address the psychological mechanisms that cause noise, and show how they relate to the specific decision hygiene techniques we have discussed The goal of judgment is accuracy, not individual expression. This statement is our candidate for the first principle of decision hygiene in judgment. It reflects the narrow, specific way we have defined judgment in this book. We have shown that stable pattern noise is a large component of system noise and that it is a direct consequence of individual differences, of judgment personalities that lead different people to form different views of the same problem. This observation leads to a conclusion that will be as unpopular as it is inescapable: judgment is not the place to express your individuality. To be clear, personal values, individuality, and creativity are needed, even essential, in many phases of thinking and decision making, including the choice of goals, the formulation of novel ways to approach a problem, and the generation of options. But when it comes to making a judgment about these options, expressions of individuality are a source of noise. When the goal is accuracy and you expect others to agree with you, you should also consider what other competent judges would think if they were in your place. A radical application of this principle is the replacement of judgment with rules or algorithms. Algorithmic evaluation is guaranteed to eliminate noise—indeed, it is the only approach that can eliminate noise completely. Algorithms are already in use in many important domains, and their role is increasing. But it is unlikely that algorithms will replace human judgment in the final stage of important decisions—and we consider this good news. However, judgment can be improved, by both the appropriate use of algorithms and the adoption of approaches that make decisions less dependent on the idiosyncrasies of one professional. We have seen, for instance, how decision guidelines can help constrain the discretion of judges or promote homogeneity in the diagnoses of physicians and thus reduce noise and improve decisions. Think statistically, and take the outside view of the case. We say that a judge takes the outside view of a case when she considers it as a member of a reference class of similar cases rather than as a unique problem. This approach diverges from the default mode of thinking, which focuses firmly on the case at hand and embeds it in a causal story. When people apply their unique experiences to form a unique view of the case, the result is pattern noise. The outside view is a remedy for this problem: professionals who share the same reference class will be less noisy. In addition, the outside view often yields valuable insights. The outside-view principle favors the anchoring of predictions in the statistics of similar cases. It also leads to the recommendation that predictions should be moderate (the technical term is regressive; see appendix C). Attention to the wide range of past outcomes and to their limited predictability should help decision makers calibrate their confidence in their judgments. People cannot be faulted for failing to predict the unpredictable, but they can be blamed for a lack of predictive humility. Structure judgments into several independent tasks. This divide-and-conquer principle is made necessary by the psychological mechanism we have described as excessive coherence, which causes people to distort or ignore information that does not fit a preexisting or emerging story. Overall accuracy suffers when impressions of distinct aspects of a case contaminate each other. For an analogy, think of what happens to the evidentiary value of a set of witnesses when they are allowed to communicate. People can reduce excessive coherence by breaking down the judgment problem into a series of smaller tasks. This technique is analogous to the practice of structured interviews, in which interviewers evaluate one trait at a time and score it before moving to the next one. The principle of structuring inspires diagnostic guidelines, such as the Apgar score. It is also at the heart of the approach we have called the mediating assessments protocol. This protocol breaks down a complex judgment into multiple fact-based assessments and aims to ensure that each one is evaluated independently of the others. Whenever possible, independence is protected by assigning assessments to different teams and minimizing communication among them. Resist premature intuitions. We have described the internal signal of judgment completion that gives decision makers confidence in their judgment. The unwillingness of decision makers to give up this rewarding signal is a key reason for the resistance to the use of guidelines and algorithms and other rules that tie their hands. Decision makers clearly need to be comfortable with their eventual choice and to attain the rewarding sense of intuitive confidence. But they should not grant themselves this reward prematurely. An intuitive choice that is informed by a balanced and careful consideration of the evidence is far superior to a snap judgment. Intuition need not be banned, but it should be informed, disciplined, and delayed. This principle inspires our recommendation to sequence the information: professionals who make judgments should not be given information that they don’t need and that could bias them, even if that information is accurate. In forensic science, for example, it is good practice to keep examiners unaware of other information about a suspect. Control of discussion agendas, a key element of the mediating assessments protocol, also belongs here. An efficient agenda will ensure that different aspects of the problem are considered separately and that the formation of a holistic judgment is delayed until the profile of assessments is complete. Obtain independent judgments from multiple judges, then consider aggregating those judgments. The requirement of independence is routinely violated in the procedures of organizations, notably in meetings in which participants’ opinions are shaped by those of others. Because of cascade effects and group polarization, group discussions often increase noise. The simple procedure of collecting participants’ judgments before the discussion both reveals the extent of noise and facilitates a constructive resolution of differences. Averaging independent judgments is guaranteed to reduce system noise (but not bias). A single judgment is a sample of one, drawn from the population of all possible judgments; and increasing sample size improves the precision of estimates. The advantage of averaging is further enhanced when judges have diverse skills and complementary judgment patterns. The average of a noisy group may end up being more accurate than a unanimous judgment. Favor relative judgments and relative scales. Relative judgments are less noisy than absolute ones, because our ability to categorize objects on a scale is limited, while our ability to make pairwise comparisons is much better. Judgment scales that call for comparisons will be less noisy than scales that require absolute judgments. For example, a case scale requires judges to locate a case on a scale that is defined by instances familiar to everyone. The decision hygiene principles we have just listed are applicable not only to recurrent judgments but also to one-off major decisions, or what we call singular decisions. The existence of noise in singular decisions may seem counterintuitive: by definition, there is no variability to measure if you decide only once. Yet noise is there, causing errors. The noise in a team of shooters is invisible if we see only the first shooter in action, but the scatter would become apparent if we saw the other shooters. Similarly, the best way to think about singular judgments is to treat them as recurrent judgments that are made only once. That is why decision hygiene should improve them, too. Enforcing decision hygiene can be thankless. Noise is an invisible enemy, and a victory against an invisible enemy can only be an invisible victory. But like physical health hygiene, decision hygiene is vital. After a successful operation, you like to believe that it is the surgeon’s skill that saved your life—and it did, of course—but if the surgeon and all the personnel in the operating room had not washed their hands, you might be dead. There may not be much glory to be gained in hygiene, but there are results.
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 374 · Location 5367
</div>
<div class="noteText">
    How Much Noise? Of course, the battle against noise is not the only consideration for decision makers and organizations. Noise may be too costly to reduce: a high school could eliminate noise in grading by having five teachers read each and every paper, but that burden is hardly justified. Some noise may be inevitable in practice, a necessary side effect of a system of due process that gives each case individualized consideration, that does not treat people like cogs in a machine, and that grants decision makers a sense of agency. Some noise may even be desirable, if the variation it creates enables a system to adapt over time—as when noise reflects changing values and goals and triggers a debate that leads to change in practice or in the law. Perhaps most importantly, noise-reduction strategies may have unacceptable downsides. Many concerns about algorithms are overblown, but some are legitimate. Algorithms may produce stupid mistakes that a human would never make, and therefore lose credibility even if they also succeed in preventing many errors that humans do make. They may be biased by poor design or by training on inadequate data. Their facelessness may inspire distrust. Decision hygiene practices also have their downsides: if poorly managed, they risk bureaucratizing decisions and demoralizing professionals who feel their autonomy is being undermined. All these risks and limitations deserve full consideration. However, whether an objection to noise reduction makes sense depends on the particular noise-reduction strategy that is under discussion. An objection to aggregating judgments—perhaps on the ground that it is too costly—may not apply to the use of guidelines. To be sure, whenever the costs of noise reduction exceed its benefits, it should not be pursued. Once the cost-benefit calculation is made, it may reveal an optimal level of noise that is not zero. The problem is that in the absence of noise audits, people are unaware of how much noise there is in their judgments. When that is the case, invoking the difficulty of reducing noise is nothing but an excuse not to measure it. Bias leads to errors and unfairness. Noise does too—and yet, we do a lot less about it. Judgment error may seem more tolerable when it is random than when we attribute it to a cause; but it is no less damaging. If we want better decisions about things that matter, we should take noise reduction seriously.
</div><div class="sectionHeading">
    Appendix A: How to Conduct a Noise Audit
</div><div class="noteHeading">
    Highlight(<span class="highlight_blue">blue</span>) - Page 379 · Location 5398
</div>
<div class="noteText">
    How to Conduct a Noise Audit This appendix provides a practical guide for conducting a noise audit. You should read it from the perspective of a consultant who has been engaged by an organization to examine the quality of the professional judgments its employees produce by conducting a noise audit in one of its units. As implied by its name, the focus of the audit is the prevalence of noise. However, a well-conducted audit will provide valuable information about biases, blind spots, and specific deficiencies in the training of employees and in the supervision of their work. A successful audit should stimulate changes in the operations of the unit, including in the doctrine that guides professionals’ judgments, the training they receive, the tools they use to support their judgments, and the routine supervision of their work. If the effort is considered successful, it may be extended to other units of the organization. A noise audit requires a substantial amount of work and much attention to detail because its credibility will surely be questioned if its findings reveal significant flaws. Every detail of the cases and the procedure should therefore be considered with hostile scrutiny in mind. The process we describe aims to reduce opposition by enlisting the professionals who are the most significant potential critics of the audit to be its authors. Alongside the consultant (who may be external or internal), the relevant cast of characters includes the following: • Project team. The project team will be responsible for all phases of the study. If the consultants are internal, they will form the core of the project team. If the consultants are external, an internal project team will work closely with them. This will ensure that people in the company view the audit as their project and consider the consultants as playing a supporting role. In addition to the consultants who administer the collection of data, analyze the results, and prepare a final report, the project team should include subject matter experts who can construct the cases that the judges will assess. All the members of the project team should have high professional credibility. • Clients. A noise audit will only be useful if it leads to significant changes, which requires early involvement of the leadership of the organization, which is the “client” of the project. You can expect clients to be initially skeptical about the prevalence of noise. This initial skepticism is actually an advantage if it is accompanied by an open-minded attitude, curiosity about the results of the audit, and a commitment to remedy the situation if the consultant’s pessimistic expectations are confirmed. • Judges. The clients will designate one or more units to be audited. The selected unit should consist of a substantial number of “judges,” the professionals who make similar judgments and decisions on behalf of the company. The judges should be effectively interchangeable; i.e., if one person was unavailable to handle a case, another would be assigned to it and expected to arrive at a similar judgment. The examples that introduced this book were sentencing decisions of federal judges and the setting of risk premiums and claims reserves in an insurance company. For a noise audit, it is best to select a judgment task that (1) can be completed on the basis of written information, and (2) is expressed numerically (e.g., in dollars, probabilities, or ratings). • Project manager. A high-level manager in the administrative staff should be designated as project manager. Specific professional expertise is not required for that task. However, a high position in the organization is of practical significance in overcoming administrative hurdles and is also a demonstration of the importance that the company attaches to the project. The task of the project manager is to provide administrative support to facilitate all phases of the project, including the preparation of the final report and the communication of its conclusions to the leadership of the company. Construction of Case Materials The subject matter experts who are part of the project team should have recognized expertise in the task of the unit (e.g., setting premiums for risks or evaluating the potential of possible investments). They will be in charge of developing the cases that will be used in the audit. Designing a credible simulation of the judgments professionals make on the job is a delicate task—especially given the scrutiny that the study will undergo if it reveals serious problems. The team must consider this question: if the results of our simulation indicate a high level of noise, will people in the company accept that there is noise in the actual judgments of the unit? The noise audit is only worth carrying out if the answer is a clear yes. There is more than one way to achieve a positive response. The noise audit of sentencing described in chapter 1 summarized each case by a brief schematic list of relevant attributes and obtained assessments of sixteen cases in ninety minutes. The noise audit in the insurance company described in chapter 2 used detailed and realistic summaries of complex cases. Findings of high noise in both instances provided acceptable evidence because of the argument that if much disagreement was found in simplified cases, noise could only be worse in real cases. A questionnaire should be prepared for each case, to provide a deeper understanding of the reasoning that led each judge to a judgment of that case. The questionnaire should be administered only after the completion of all cases. It should include: • Open questions about the key factors that led the participant to her response. • A list of the facts of the case, allowing the participant to rate their importance. • Questions that call for an “outside view” of the category to which the case belongs. For instance, if the cases call for dollar valuations, participants should provide an estimate of how much below or above average the case is compared to all valuations for cases of the same category. Prelaunch Meeting with Executives When the case materials to be used in the audit are assembled, a meeting should be scheduled in which the project team will present the audit to the leadership of the company. The discussion in that meeting should consider possible outcomes of the study, including a finding of unacceptable system noise. The purpose of the meeting is to hear objections to the planned study and to obtain from the leadership a commitment to accept its results, whatever they are: there is no point moving on to the next stage without such a commitment. If serious objections are raised, the project team may be required to improve the case materials and try again. Once the executives accept the design of the noise audit, the project team should ask them to state their expectations about the results of the study. They should discuss questions such as: • “What level of disagreement do you expect between a randomly selected pair of answers to each case?” • “What is the maximum level of disagreement that would be acceptable from a business perspective?” • “What is the estimated cost of getting an evaluation wrong in either direction (too high or low) by a specified amount (e.g., 15%)?” The answers to these questions should be documented to ensure that they are remembered and believed when the actual results of the audit come in. Administration of the Study The managers of the audited unit should be, from the beginning, informed in general terms that their unit has been selected for special study. However, it is important that the term noise audit not be used to describe the project. The words noise and noisy should be avoided, especially as descriptions of people. A neutral term such as decision-making study should be used instead. The managers of the unit will be immediately in charge of the data collection and responsible for briefing the participants about the task, with the participation of the project manager and members of the project team. The intent of the exercise should be described to the participants in general terms, as in “The organization is interested in how [decision makers] reach their conclusions.” It is essential to reassure the professionals who participate in the study that individual answers will not be known to anyone in the organization, including the project team. If necessary, an outside firm may be hired to anonymize the data. It is also important to stress that there will be no specific consequences for the unit, which was merely selected as representative of units that perform judgment tasks on behalf of the organization. To ensure the credibility of the results, all qualified professionals in the unit should participate in the study. The allocation of half a working day to the exercise will help convince the participants of its importance. All participants should complete the exercise at the same time, but they should be kept physically separate and asked not to communicate while the study is in progress. The project team will be available to answer questions during the study. Analyses and Conclusions The project team will be in charge of the statistical analyses of the multiple cases evaluated by each participant, including the measurement of the overall amount of noise and its constituents, level noise and pattern noise. If the case materials allow it, it will also identify statistical biases in the responses. The project team will have the equally important task of trying to understand the sources of variability in judgments by examining responses to the questionnaire in which participants explained their reasoning and identified the facts that most influenced their decisions. Focusing mainly on extreme responses at both ends of the distribution, the team will search for patterns in the data. It will look for indications of possible deficiencies in the training of employees, the procedures of the organization, and the information that it provides to its employees. The consultant and the internal project team will work together to develop tools and procedures that apply principles of decision hygiene and debiasing to improve the judgments and decisions made in the unit. This step of the process is likely to extend over several months. In parallel, the consultant and the professional team will also prepare a report on the project, which they will present to the leadership of the organization. At this point, the organization will have carried out a sample noise audit in one of its units. If the effort is considered successful, the executive team may decide on a broader effort to evaluate and improve the quality of the judgments and decisions that are produced in the organization.
</div>
        </div>
    </body>
</html>